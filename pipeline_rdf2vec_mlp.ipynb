{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060e16b6",
   "metadata": {},
   "source": [
    "This file describes how RDF2Vec & a MLP were used to predict the outside temperature, based on the power usage of a heat pump. The power usage measurements have been stored in a graph using the SAREF ontology to represent the relations between the measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675ed8e",
   "metadata": {},
   "source": [
    "HARDCODED:\n",
    "    hasValue, as indication of where the values can be retrieved from the graph, including the spacing in the entire line when the value is retrieved\n",
    "    the range of the scientific notation \n",
    "    removal of the trailing digits\n",
    "    \n",
    "    power_usage_emb\n",
    "    tempC_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c2dc6",
   "metadata": {},
   "source": [
    "## PART1: Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba00eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_numbers_in_graph(filename=\"test_files/res1_hp_temp_kg_SMALL.ttl\", new_fn=None):\n",
    "    if new_fn == None:\n",
    "        new_fn = filename[:-4]+'_CHANGED.ttl'\n",
    "    new_lines = []\n",
    "    with open(filename) as ttl_data:\n",
    "        for line in ttl_data.readlines():\n",
    "            if 'hasValue' in line: #take the value out of the line (and keep the prefix and suffix for later)\n",
    "                part1 = line[:19]\n",
    "                value = line[19:-3]\n",
    "                part3 = line[-3:]\n",
    "                if value[-3:] == '+00': #change scientific notation into regular notation (only these three were relevant)\n",
    "                    value = value[:-4]\n",
    "                elif value[-3:] == '+01':\n",
    "                    value = str(float(value[:-4]) * 10)\n",
    "                elif value[-3:] == '-01':\n",
    "                    value = str(float(value[:-4]) / 10)\n",
    "                else:\n",
    "                    print(\"STRANGE NUMBER: \", value) #catch if an \"out of bounds\" scientific notation has been encountered\n",
    "                    exit()\n",
    "                new_value = '\"'+value.replace('.', ',')+'\"' #replace '.' with ',' to make sure the string stays a string (and not a float)\n",
    "                for i in range(1,7): #remove trailing digits caused by conversion\n",
    "                    if new_value.endswith(\"000\"+str(i)+\"\\\"\"):\n",
    "                        new_value = new_value[:new_value.find(\"000\")]+'\"'\n",
    "                new_lines.append(part1+new_value+part3)\n",
    "            else:\n",
    "                new_lines.append(line) #append non-value lines to the new file\n",
    "\n",
    "    with open(new_fn, 'w') as ttl_data:\n",
    "        for line in new_lines:\n",
    "            ttl_data.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfee7de",
   "metadata": {},
   "source": [
    "Create the entities file **manually**, with the entities (nodes from the graph) we want to transform and their corresponding targets for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3080ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change numbers to work\n",
    "def change_numbers_in_entities(entities_fn=\"test_files/res1_entities_SMALL.tsv\", graph_fn=\"test_files/res1_hp_temp_kg_SMALL_CHANGED.ttl\", new_entities_fn=None):\n",
    "    if new_entities_fn == None:\n",
    "        new_entities_fn = entities_fn[:-4]+'_CHANGED.tsv'\n",
    "    ttl_entities = get_entities_from_ttl(graph_fn)\n",
    "    new_lines = []\n",
    "    collected_powers = []\n",
    "    skipped_entities = {'double': 0, 'not_in_graph': 0}\n",
    "    \n",
    "    with open(entities_fn) as tsv_data:\n",
    "        new_lines.append(tsv_data.readline())\n",
    "        for line in tsv_data.readlines():\n",
    "            line = line.split('\\t')\n",
    "            power_usage = line[0]\n",
    "            temp = line[1]\n",
    "            new_power = str(power_usage).replace('.', ',')\n",
    "            if new_power in ttl_entities: #check if the entity exists in the graph\n",
    "                if not new_power in collected_powers: #only add each entity once, to not break one-to-one embedding\n",
    "                    new_lines.append('\\t'.join([new_power,temp]))\n",
    "                    collected_powers.append(new_power)\n",
    "                else:\n",
    "#                     print(\"skipped entity: DOUBLE ENTITY\")\n",
    "                    skipped_entities['double'] += 1\n",
    "            else:\n",
    "#                 print(\"skipped entity: not available in graph\")\n",
    "                skipped_entities['not_in_graph'] += 1\n",
    "    print('skipped', skipped_entities['double'], 'DOUBLE entities and', skipped_entities['not_in_graph'], 'UNAVAILABLE entities.')\n",
    "    with open(new_entities_fn, 'w') as tsv_data:\n",
    "        for line in new_lines:\n",
    "            tsv_data.write(line)\n",
    "\n",
    "def get_entities_from_ttl(ttl_filename):\n",
    "    new_values_ttl = []\n",
    "\n",
    "    with open(ttl_filename) as ttl_data:\n",
    "        for line in ttl_data.readlines():\n",
    "            if 'hasValue' in line:\n",
    "                value = line[19:-3]\n",
    "                new_values_ttl.append(value[1:-1])\n",
    "    return new_values_ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3d37f",
   "metadata": {},
   "source": [
    "## Part 2: Creating the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5680297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyrdf2vec import RDF2VecTransformer\n",
    "from pyrdf2vec.embedders import Word2Vec\n",
    "from pyrdf2vec.graphs import KG\n",
    "from pyrdf2vec.walkers import RandomWalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f102d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(entities_fn=\"test_files/res1_entities_SMALL_CHANGED.tsv\", kg_fn=\"test_files/res1_hp_temp_kg_SMALL_CHANGED.ttl\", new_entities_fn=None, entities_column_name=\"power_usage\", reverse=False):\n",
    "    if new_entities_fn == None:\n",
    "        new_entities_fn = entities_fn[:-4]+'_embeddings.tsv'\n",
    "    data = pd.read_csv(entities_fn, sep=\"\\t\")\n",
    "    \n",
    "    entities = [entity for entity in data[entities_column_name]]\n",
    "    transformer = RDF2VecTransformer(\n",
    "        Word2Vec(epochs=1),\n",
    "        walkers=[RandomWalker(4, 10, with_reverse=reverse, n_jobs=8, md5_bytes=None)],\n",
    "        verbose=1\n",
    "    )\n",
    "    kg = KG(location=kg_fn)\n",
    "    embeddings, literals = transformer.fit_transform(kg, entities)\n",
    "\n",
    "    new_emb = []\n",
    "    for embedding in embeddings:\n",
    "        new_emb.append(embedding.tolist())\n",
    "\n",
    "    data[entities_column_name+'_emb'] = new_emb\n",
    "    data.to_csv(new_entities_fn, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ea0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walks(entities_fn=\"test_files/res1_entities_SMALL_CHANGED.tsv\", kg_fn=\"test_files/res1_hp_temp_kg_SMALL_CHANGED.ttl\", entities_column_name=\"power_usage\"):\n",
    "    data = pd.read_csv(entities_fn, sep=\"\\t\")\n",
    "    \n",
    "    entities = [entity for entity in data[entities_column_name]]\n",
    "    transformer = RDF2VecTransformer(\n",
    "        Word2Vec(epochs=1),\n",
    "        walkers=[RandomWalker(4, 10, with_reverse=True, n_jobs=8, md5_bytes=None)],\n",
    "        verbose=2\n",
    "    )\n",
    "    kg = KG(location=kg_fn)\n",
    "\n",
    "    print(transformer.get_walks(kg,entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafa097",
   "metadata": {},
   "source": [
    "## part 3a: transform entities .tsv to pytorch training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b762ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1a26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def show_hp_emb_temp_values(power_usage_emb, tempC_average):\n",
    "    print(power_usage_emb, tempC_average)\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        hp, temp = sample['power_usage_emb'], sample['tempC_average']\n",
    "        return {'power_usage_emb': torch.from_numpy(hp),\n",
    "                'tempC_average': torch.from_numpy(temp)}\n",
    "\n",
    "\n",
    "class HP_emb_TempDataset(Dataset):\n",
    "    \"\"\"Dataset containing the Heat Pump power consumption values and the temperature at that time.\"\"\"\n",
    "\n",
    "    def __init__(self, tsv_file='res1_entities_embeddings.tsv', train=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the tsv file with two columns, hp consumption and temp.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        tsv_file = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "        if train:\n",
    "            self.hptempvalues = tsv_file[:int(len(tsv_file)*0.8)].reset_index() #[power_usage, tempC_average]\n",
    "        else:\n",
    "            self.hptempvalues = tsv_file[int(len(tsv_file)*0.8):].reset_index() #[power_usage, tempC_average]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hptempvalues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # print(\"idx:\", idx)\n",
    "        sample = {'power_usage_emb': np.array([float(x.strip(' []')) for x in self.hptempvalues['power_usage_emb'][idx].split(',')]), 'tempC_average': np.array([float(self.hptempvalues['tempC_average'][idx])]) }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Helper function to show a batch\n",
    "def show_hp_emb_temp_batch(sample_batched):\n",
    "    \"\"\"Show power usage of heat pump and temp\"\"\"\n",
    "    power_batch, temp_batch = \\\n",
    "            sample_batched['power_usage_emb'], sample_batched['tempC_average']\n",
    "    batch_size = len(power_batch)\n",
    "\n",
    "    print('Batch from dataloader')\n",
    "    for i in range(batch_size):\n",
    "        print(power_batch[i,:], temp_batch[i,:])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     hp_temp_dataset = HP_emb_TempDataset()\n",
    "\n",
    "#     for i in range(len(hp_temp_dataset)):\n",
    "#         sample = hp_temp_dataset[i]\n",
    "\n",
    "#         print(i, sample['power_usage_emb'].shape, sample['tempC_average'].shape)\n",
    "#         show_hp_emb_temp_values(**sample)\n",
    "\n",
    "#         if i > 3:\n",
    "#             break\n",
    "#     print(len((hp_temp_dataset[0]['power_usage_emb']).tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b47251",
   "metadata": {},
   "source": [
    "## Part 3b: Training the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d51f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cad5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_prediction(dataset_fn = 'res1_entities_embeddings.tsv', results_fn = 'results.txt'):\n",
    "    training_data = HP_emb_TempDataset(tsv_file=dataset_fn, train=True, transform=ToTensor())\n",
    "    test_data = HP_emb_TempDataset(tsv_file=dataset_fn, train=False, transform=ToTensor())\n",
    "\n",
    "    batch_size = 4\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    for sample in test_dataloader:\n",
    "        X = sample['power_usage_emb']\n",
    "        y = sample['tempC_average']\n",
    "        print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "        print(\"Shape of y: \", y.shape, y.dtype)\n",
    "        break\n",
    "\n",
    "    # exit()\n",
    "\n",
    "    # Get cpu or gpu device for training.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "\n",
    "    # Define model\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(100, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            logits = self.linear_relu_stack(x)\n",
    "            return logits\n",
    "\n",
    "    model = NeuralNetwork().to(device)\n",
    "    model = model.float()\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "    def train(dataloader, model, loss_fn, optimizer):\n",
    "        size = len(dataloader.dataset)\n",
    "        model.train()\n",
    "        for batch, sample in enumerate(dataloader):\n",
    "            X = sample['power_usage_emb']\n",
    "            y = sample['tempC_average']\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred.float(), y.float())\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                # print(\"x\", X[0], \"y\", torch.round(y[0]), \"pred\", torch.round(pred[0]))\n",
    "                # print(\"x\", X[0], \"y\", torch.round(y[0]), \"pred\", torch.round(pred[0]))\n",
    "\n",
    "\n",
    "    def test(dataloader, model, loss_fn):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        model.eval()\n",
    "        test_loss, within_2, within_5 = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for sample in dataloader:\n",
    "                X = sample['power_usage_emb']\n",
    "                y = sample['tempC_average']\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X.float())\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                for i in range(0, len(y)):\n",
    "                    if abs(int(y[i].item()) - int(pred[i].item())) < 3:\n",
    "                        within_2 += 1\n",
    "                        within_5 += 1\n",
    "                    elif abs(int(y[i].item()) - int(pred[i].item())) < 6:\n",
    "                        within_5 += 1\n",
    "        test_loss /= num_batches\n",
    "        within_2 = within_2 / size\n",
    "        within_5 = (within_2 + within_5) / size\n",
    "        # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        print(f\"Test Error: \\n accurate within 2 degrees: {(100*within_2):>0.1f}%, \\n accurate within 5 degrees: {(100*within_5):>0.1f}%, \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "        return within_2\n",
    "\n",
    "\n",
    "    epochs = 15\n",
    "    model = model.float()\n",
    "    res = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        res.append(test(test_dataloader, model, loss_fn))\n",
    "    print(\"Done!\")\n",
    "    print(res)\n",
    "    with open(results_fn, 'w') as results_file:\n",
    "        for r in res:\n",
    "            results_file.write(str(r)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29209fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    graph_name = \"test_files/res1_hp_temp_kg_SMALL.ttl\"\n",
    "    entities_name =\"test_files/res1_entities_SMALL.tsv\"\n",
    "\n",
    "    #the names of the changed graph and entities files\n",
    "    graph_c = \"res1_g_final_test.ttl\"\n",
    "    entities_c = \"res1_e_final_test.tsv\"\n",
    "\n",
    "    # #updated entities file with embeddings\n",
    "    entities_emb = \"res1_emb_final_test.tsv\"\n",
    "\n",
    "    change_numbers_in_graph(filename=graph_name, \n",
    "                            new_fn=graph_c)\n",
    "    change_numbers_in_entities(entities_fn=entities_name, \n",
    "                               graph_fn=graph_c, \n",
    "                               new_entities_fn=entities_c)\n",
    "\n",
    "    make_embeddings(entities_fn=entities_c, \n",
    "                    kg_fn=graph_c, \n",
    "                    new_entities_fn=entities_emb,\n",
    "                    entities_column_name=\"power_usage\",reverse=True) #CHANGE REVERSE TO FALSE?!\n",
    "\n",
    "    perform_prediction(dataset_fn=entities_emb, results_fn=\"results_test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b37a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRYING TO SEE THE PATHS\n",
    "\n",
    "# graph_name = \"res1_hp_temp_kg.ttl\"\n",
    "# entities_name =\"res1_entities_SMALL.tsv\"\n",
    "\n",
    "# #the names of the changed graph and entities files\n",
    "# graph_c = \"res1_g_final_test.ttl\"\n",
    "# entities_c = \"res1_e_final_test.tsv\"\n",
    "\n",
    "# get_walks(entities_fn=entities_c, \n",
    "#                 kg_fn=graph_c, \n",
    "#                 entities_column_name=\"power_usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0218b21a-64b2-40a5-b4d1-6c48a95f6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 0 DOUBLE entities and 2 UNAVAILABLE entities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 41.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 272 walks for 6 entities (0.2382s)\n",
      "Fitted 272 walks (0.0063s)\n",
      "Shape of X [N, C, H, W]:  torch.Size([2, 100])\n",
      "Shape of y:  torch.Size([2, 1]) torch.float64\n",
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 99.585487  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 110.006612 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 99.519966  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.937892 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 99.454544  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.869192 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 99.389175  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.800535 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 99.323837  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.731961 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 99.258537  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.663450 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 99.193291  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.595006 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 99.128143  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.526653 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 99.063110  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.458337 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 98.998108  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.390068 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 98.933189  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.321863 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 98.868309  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.253729 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 98.803459  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.185624 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 98.738617  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.117492 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 98.673782  [    0/    4]\n",
      "Test Error: \n",
      " accurate within 2 degrees: 0.0%, \n",
      " accurate within 5 degrees: 0.0%, \n",
      " Avg loss: 109.049427 \n",
      "\n",
      "Done!\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a421d7b-46bb-4951-afeb-eab5b927ac51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
